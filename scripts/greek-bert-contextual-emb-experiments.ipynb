{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T21:49:59.047457Z",
     "start_time": "2025-08-23T21:49:57.754975Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# import basic libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import random\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.manifold import TSNE\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from collections import Counter\n",
    "import plotly.io as pio\n",
    "#import pyperclip\n",
    "from sklearn.cluster import KMeans\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import unicodedata\n",
    "import duckdb\n"
   ],
   "id": "aa7c961cfbf8ce2c",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T21:50:01.362947Z",
     "start_time": "2025-08-23T21:50:01.347855Z"
    }
   },
   "cell_type": "code",
   "source": "conn = duckdb.connect('/srv/data/greek/grela.duckdb', read_only=True)",
   "id": "9a8bf478a12801a8",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T21:50:01.933162Z",
     "start_time": "2025-08-23T21:50:01.906186Z"
    }
   },
   "cell_type": "code",
   "source": [
    "enemy_annotations = pd.read_csv(\"../data/enemy-annotations - sentences.csv\")\n",
    "enemy_annotations.head(5)"
   ],
   "id": "841c535e01fb210e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   Unnamed: 0                sentence_id grela_source             grela_id  \\\n",
       "0           4   lagt_tlg4090.tlg001_8695         lagt  lagt_tlg4090.tlg001   \n",
       "1           5  lagt_tlg4090.tlg001_38952         lagt  lagt_tlg4090.tlg001   \n",
       "2          19  lagt_tlg4090.tlg001_60254         lagt  lagt_tlg4090.tlg001   \n",
       "3          29   lagt_tlg0317.tlg001_2231         lagt  lagt_tlg0317.tlg001   \n",
       "4          41    lagt_tlg2035.tlg131_764         lagt  lagt_tlg2035.tlg131   \n",
       "\n",
       "                author                     title  not_before  not_after  \\\n",
       "0  Cyril of Alexandria          In XII Prophetas       412.0      444.0   \n",
       "1  Cyril of Alexandria          In XII Prophetas       412.0      444.0   \n",
       "2  Cyril of Alexandria          In XII Prophetas       412.0      444.0   \n",
       "3         Acta Joannis              Acta Joannis       101.0      200.0   \n",
       "4           Athanasius  Oratio II contra Arianos       330.0      373.0   \n",
       "\n",
       "  lagt_tlg_epithet lagt_genre lagt_provenience  date_random  token_count  \\\n",
       "0   ['Theologici']         []        christian        425.0       482822   \n",
       "1   ['Theologici']         []        christian        425.0       482822   \n",
       "2   ['Theologici']         []        christian        425.0       482822   \n",
       "3               []         []        christian        180.0        23640   \n",
       "4   ['Theologici']         []        christian        358.0        37973   \n",
       "\n",
       "                                                text polemical category  \\\n",
       "0  εἶτα δέον πάντας τούς ἐξ Ἰσραήλ ἐχθρούς ἡγεῖσθ...                  1   \n",
       "1  μέθῃ δή οὖν καί σκότῳ καταληφθήσεσθαί φησι τού...                  1   \n",
       "2  . . . . αὐτό . . αὐτῶ . τοαυτό . . πεφρασμένον...                  1   \n",
       "3  ὑμέρας ‖ μέρος ‖ ἤ ‖ ἐν παῤ αἰτῶ Δ ἀλλά πυρί α...                  1   \n",
       "4  ἕως ἄν . .( .) . θῶ τούς ἐχθρούς σου ὑποπόδιον...                  1   \n",
       "\n",
       "                        notes  \n",
       "0  against Jews, δυσσεβείας    \n",
       "1                         NaN  \n",
       "2                         NaN  \n",
       "3                         NaN  \n",
       "4                         NaN  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>grela_source</th>\n",
       "      <th>grela_id</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>not_before</th>\n",
       "      <th>not_after</th>\n",
       "      <th>lagt_tlg_epithet</th>\n",
       "      <th>lagt_genre</th>\n",
       "      <th>lagt_provenience</th>\n",
       "      <th>date_random</th>\n",
       "      <th>token_count</th>\n",
       "      <th>text</th>\n",
       "      <th>polemical category</th>\n",
       "      <th>notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>lagt_tlg4090.tlg001_8695</td>\n",
       "      <td>lagt</td>\n",
       "      <td>lagt_tlg4090.tlg001</td>\n",
       "      <td>Cyril of Alexandria</td>\n",
       "      <td>In XII Prophetas</td>\n",
       "      <td>412.0</td>\n",
       "      <td>444.0</td>\n",
       "      <td>['Theologici']</td>\n",
       "      <td>[]</td>\n",
       "      <td>christian</td>\n",
       "      <td>425.0</td>\n",
       "      <td>482822</td>\n",
       "      <td>εἶτα δέον πάντας τούς ἐξ Ἰσραήλ ἐχθρούς ἡγεῖσθ...</td>\n",
       "      <td>1</td>\n",
       "      <td>against Jews, δυσσεβείας</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>lagt_tlg4090.tlg001_38952</td>\n",
       "      <td>lagt</td>\n",
       "      <td>lagt_tlg4090.tlg001</td>\n",
       "      <td>Cyril of Alexandria</td>\n",
       "      <td>In XII Prophetas</td>\n",
       "      <td>412.0</td>\n",
       "      <td>444.0</td>\n",
       "      <td>['Theologici']</td>\n",
       "      <td>[]</td>\n",
       "      <td>christian</td>\n",
       "      <td>425.0</td>\n",
       "      <td>482822</td>\n",
       "      <td>μέθῃ δή οὖν καί σκότῳ καταληφθήσεσθαί φησι τού...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19</td>\n",
       "      <td>lagt_tlg4090.tlg001_60254</td>\n",
       "      <td>lagt</td>\n",
       "      <td>lagt_tlg4090.tlg001</td>\n",
       "      <td>Cyril of Alexandria</td>\n",
       "      <td>In XII Prophetas</td>\n",
       "      <td>412.0</td>\n",
       "      <td>444.0</td>\n",
       "      <td>['Theologici']</td>\n",
       "      <td>[]</td>\n",
       "      <td>christian</td>\n",
       "      <td>425.0</td>\n",
       "      <td>482822</td>\n",
       "      <td>. . . . αὐτό . . αὐτῶ . τοαυτό . . πεφρασμένον...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29</td>\n",
       "      <td>lagt_tlg0317.tlg001_2231</td>\n",
       "      <td>lagt</td>\n",
       "      <td>lagt_tlg0317.tlg001</td>\n",
       "      <td>Acta Joannis</td>\n",
       "      <td>Acta Joannis</td>\n",
       "      <td>101.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>christian</td>\n",
       "      <td>180.0</td>\n",
       "      <td>23640</td>\n",
       "      <td>ὑμέρας ‖ μέρος ‖ ἤ ‖ ἐν παῤ αἰτῶ Δ ἀλλά πυρί α...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>lagt_tlg2035.tlg131_764</td>\n",
       "      <td>lagt</td>\n",
       "      <td>lagt_tlg2035.tlg131</td>\n",
       "      <td>Athanasius</td>\n",
       "      <td>Oratio II contra Arianos</td>\n",
       "      <td>330.0</td>\n",
       "      <td>373.0</td>\n",
       "      <td>['Theologici']</td>\n",
       "      <td>[]</td>\n",
       "      <td>christian</td>\n",
       "      <td>358.0</td>\n",
       "      <td>37973</td>\n",
       "      <td>ἕως ἄν . .( .) . θῶ τούς ἐχθρούς σου ὑποπόδιον...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T21:50:02.249244Z",
     "start_time": "2025-08-23T21:50:02.247152Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "e71758538511a2a1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T21:50:07.522120Z",
     "start_time": "2025-08-23T21:50:07.168441Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# add tokens data to annotated sentences\n",
    "conn.register(\"enemy_annotations\", enemy_annotations)\n",
    "\n",
    "enemy_annotations = conn.execute(\"\"\"\n",
    "SELECT ea.*,\n",
    "       COALESCE(st.tokens, []) AS tokens\n",
    "FROM enemy_annotations ea\n",
    "LEFT JOIN (\n",
    "  SELECT\n",
    "    t.sentence_id,\n",
    "    LIST(\n",
    "      STRUCT_PACK(\n",
    "        token_id    := t.token_id,\n",
    "        token_text  := t.token_text,\n",
    "        lemma       := t.lemma,\n",
    "        pos         := t.pos,\n",
    "        char_start  := t.char_start,\n",
    "        char_end    := t.char_end,\n",
    "        sentence_id := t.sentence_id\n",
    "      )\n",
    "      ORDER BY t.token_id\n",
    "    ) AS tokens\n",
    "  FROM tokens t\n",
    "  SEMI JOIN enemy_annotations ea USING (sentence_id)\n",
    "  GROUP BY t.sentence_id\n",
    ") st USING (sentence_id)\n",
    "\"\"\").df()"
   ],
   "id": "ec4c42945c4c7e2a",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T21:50:07.669443Z",
     "start_time": "2025-08-23T21:50:07.648753Z"
    }
   },
   "cell_type": "code",
   "source": "enemy_annotations.head(5)",
   "id": "9340e6cf4e391a82",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   Unnamed: 0                sentence_id grela_source               grela_id  \\\n",
       "0         104  lagt_tlg2042.tlg058_10803         lagt    lagt_tlg2042.tlg058   \n",
       "1         571    lagt_tlg0061.tlg003_347         lagt    lagt_tlg0061.tlg003   \n",
       "2         489  lagt_tlg2042.tlg058_14157         lagt    lagt_tlg2042.tlg058   \n",
       "3         702   lagt_tlg2200.tlg00431_86         lagt  lagt_tlg2200.tlg00431   \n",
       "4         263    lagt_tlg2018.tlg020_667         lagt    lagt_tlg2018.tlg020   \n",
       "\n",
       "          author                      title  not_before  not_after  \\\n",
       "0         Origen  Selecta in Psalmos [Dub.]       208.0      220.0   \n",
       "1  Pseudo-Lucian       Δημοσθένους Ἐγκώμιον       201.0      300.0   \n",
       "2         Origen  Selecta in Psalmos [Dub.]       208.0      220.0   \n",
       "3       Libanius                  Oratio 31       387.0      387.0   \n",
       "4       Eusebius           Vita Constantini       325.0      339.0   \n",
       "\n",
       "                            lagt_tlg_epithet lagt_genre lagt_provenience  \\\n",
       "0                             ['Theologici']         []        christian   \n",
       "1                              ['Sophistae']         []            pagan   \n",
       "2                             ['Theologici']         []        christian   \n",
       "3                  ['Rhetorici' 'Sophistae']         []            pagan   \n",
       "4  ['Scriptores Ecclesiastici' 'Theologici']         []        christian   \n",
       "\n",
       "   date_random  token_count  \\\n",
       "0        210.0       129819   \n",
       "1        208.0         5963   \n",
       "2        210.0       129819   \n",
       "3        387.0         5508   \n",
       "4        336.0        53384   \n",
       "\n",
       "                                                text polemical category  \\\n",
       "0  Καί ἐταράχθην ἀπό φωνῆς ἐχθροῦ καί ἀπό θλίψεως...                  1   \n",
       "1             οὐκ ἐχθρῶν ἡμῖν ἔχθιστος ὁ Δημοσθένης;               None   \n",
       "2     ἅρα τούς ἐχθρούς αὐτοῦ οὐκ ὠφέλησεν ὁ Χριστός.                  1   \n",
       "3  ) διά ἐχθρῶν Ρ ἐχθροῖς δειλαίνοντες δ᾿ ἐλαύνον...                  5   \n",
       "4  τον, εἰσαεί νικῶντα τροπαίοις τε τοῖς κατ᾿ ἐχθ...                  5   \n",
       "\n",
       "          notes                                             tokens  \n",
       "0          None  [{'token_id': 163720546, 'token_text': 'Καί', ...  \n",
       "1          None  [{'token_id': 152488913, 'token_text': 'οὐκ', ...  \n",
       "2  need context  [{'token_id': 163741161, 'token_text': 'ἅρα', ...  \n",
       "3          None  [{'token_id': 145486690, 'token_text': ')', 'l...  \n",
       "4          None  [{'token_id': 145514225, 'token_text': 'τον', ...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>grela_source</th>\n",
       "      <th>grela_id</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>not_before</th>\n",
       "      <th>not_after</th>\n",
       "      <th>lagt_tlg_epithet</th>\n",
       "      <th>lagt_genre</th>\n",
       "      <th>lagt_provenience</th>\n",
       "      <th>date_random</th>\n",
       "      <th>token_count</th>\n",
       "      <th>text</th>\n",
       "      <th>polemical category</th>\n",
       "      <th>notes</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>104</td>\n",
       "      <td>lagt_tlg2042.tlg058_10803</td>\n",
       "      <td>lagt</td>\n",
       "      <td>lagt_tlg2042.tlg058</td>\n",
       "      <td>Origen</td>\n",
       "      <td>Selecta in Psalmos [Dub.]</td>\n",
       "      <td>208.0</td>\n",
       "      <td>220.0</td>\n",
       "      <td>['Theologici']</td>\n",
       "      <td>[]</td>\n",
       "      <td>christian</td>\n",
       "      <td>210.0</td>\n",
       "      <td>129819</td>\n",
       "      <td>Καί ἐταράχθην ἀπό φωνῆς ἐχθροῦ καί ἀπό θλίψεως...</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>[{'token_id': 163720546, 'token_text': 'Καί', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>571</td>\n",
       "      <td>lagt_tlg0061.tlg003_347</td>\n",
       "      <td>lagt</td>\n",
       "      <td>lagt_tlg0061.tlg003</td>\n",
       "      <td>Pseudo-Lucian</td>\n",
       "      <td>Δημοσθένους Ἐγκώμιον</td>\n",
       "      <td>201.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>['Sophistae']</td>\n",
       "      <td>[]</td>\n",
       "      <td>pagan</td>\n",
       "      <td>208.0</td>\n",
       "      <td>5963</td>\n",
       "      <td>οὐκ ἐχθρῶν ἡμῖν ἔχθιστος ὁ Δημοσθένης;</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[{'token_id': 152488913, 'token_text': 'οὐκ', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>489</td>\n",
       "      <td>lagt_tlg2042.tlg058_14157</td>\n",
       "      <td>lagt</td>\n",
       "      <td>lagt_tlg2042.tlg058</td>\n",
       "      <td>Origen</td>\n",
       "      <td>Selecta in Psalmos [Dub.]</td>\n",
       "      <td>208.0</td>\n",
       "      <td>220.0</td>\n",
       "      <td>['Theologici']</td>\n",
       "      <td>[]</td>\n",
       "      <td>christian</td>\n",
       "      <td>210.0</td>\n",
       "      <td>129819</td>\n",
       "      <td>ἅρα τούς ἐχθρούς αὐτοῦ οὐκ ὠφέλησεν ὁ Χριστός.</td>\n",
       "      <td>1</td>\n",
       "      <td>need context</td>\n",
       "      <td>[{'token_id': 163741161, 'token_text': 'ἅρα', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>702</td>\n",
       "      <td>lagt_tlg2200.tlg00431_86</td>\n",
       "      <td>lagt</td>\n",
       "      <td>lagt_tlg2200.tlg00431</td>\n",
       "      <td>Libanius</td>\n",
       "      <td>Oratio 31</td>\n",
       "      <td>387.0</td>\n",
       "      <td>387.0</td>\n",
       "      <td>['Rhetorici' 'Sophistae']</td>\n",
       "      <td>[]</td>\n",
       "      <td>pagan</td>\n",
       "      <td>387.0</td>\n",
       "      <td>5508</td>\n",
       "      <td>) διά ἐχθρῶν Ρ ἐχθροῖς δειλαίνοντες δ᾿ ἐλαύνον...</td>\n",
       "      <td>5</td>\n",
       "      <td>None</td>\n",
       "      <td>[{'token_id': 145486690, 'token_text': ')', 'l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>263</td>\n",
       "      <td>lagt_tlg2018.tlg020_667</td>\n",
       "      <td>lagt</td>\n",
       "      <td>lagt_tlg2018.tlg020</td>\n",
       "      <td>Eusebius</td>\n",
       "      <td>Vita Constantini</td>\n",
       "      <td>325.0</td>\n",
       "      <td>339.0</td>\n",
       "      <td>['Scriptores Ecclesiastici' 'Theologici']</td>\n",
       "      <td>[]</td>\n",
       "      <td>christian</td>\n",
       "      <td>336.0</td>\n",
       "      <td>53384</td>\n",
       "      <td>τον, εἰσαεί νικῶντα τροπαίοις τε τοῖς κατ᾿ ἐχθ...</td>\n",
       "      <td>5</td>\n",
       "      <td>None</td>\n",
       "      <td>[{'token_id': 145514225, 'token_text': 'τον', ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T21:50:15.237806Z",
     "start_time": "2025-08-23T21:50:08.127641Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open (\"/srv/data/enemy-christ/large-data/grouped_df.pkl\", \"rb\") as f:\n",
    "    grouped = pickle.load(f)"
   ],
   "id": "3ca9dc6eb8d810b5",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3989005/57775235.py:2: DeprecationWarning: numpy.core.numeric is deprecated and has been renamed to numpy._core.numeric. The numpy._core namespace contains private NumPy internals and its use is discouraged, as NumPy internals can change without warning in any release. In practice, most real-world usage of numpy.core is to access functionality in the public NumPy API. If that is the case, use the public NumPy API. If not, you are using NumPy internals. If you would still like to access an internal attribute, use numpy._core.numeric._frombuffer.\n",
      "  grouped = pickle.load(f)\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T21:50:15.296854Z",
     "start_time": "2025-08-23T21:50:15.291314Z"
    }
   },
   "cell_type": "code",
   "source": "grouped[\"tokens\"][0]",
   "id": "5f73bca230feebec",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[{\"lemma\": \"πινυτός\", \"token\": \"Πινυτός\", \"pos\": \"n\", \"char_start\": 2, \"char_end\": 9}, {\"lemma\": \"ἀντιγράφω\", \"token\": \"ἀντιγράφων\", \"pos\": \"v\", \"char_start\": 10, \"char_end\": 20}]'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T21:50:15.344252Z",
     "start_time": "2025-08-23T21:50:15.339839Z"
    }
   },
   "cell_type": "code",
   "source": "grouped[\"sentence_text\"][0]",
   "id": "7f6eed62cf39efab",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ὁ Πινυτός ἀντιγράφων,'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T21:50:16.900316Z",
     "start_time": "2025-08-23T21:50:15.386555Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"pranaydeeps/Ancient-Greek-BERT\")\n",
    "model = AutoModel.from_pretrained(\"pranaydeeps/Ancient-Greek-BERT\")"
   ],
   "id": "108c685dc4168b07",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/venvs/greek_venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T21:50:16.957759Z",
     "start_time": "2025-08-23T21:50:16.951605Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sent = enemy_annotations[\"text\"][1]\n",
    "sent"
   ],
   "id": "16bd54d8b6426eb8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'οὐκ ἐχθρῶν ἡμῖν ἔχθιστος ὁ Δημοσθένης;'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T21:50:17.010136Z",
     "start_time": "2025-08-23T21:50:17.002526Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_ids = tokenizer.encode(sent)\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "print(tokens)"
   ],
   "id": "2b9686d45faff284",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'ουκ', 'εχθρων', 'ημι', '##ν', 'εχ', '##θι', '##στος', 'ο', 'δημοσθενη', '##ς', ';', '[SEP]']\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T21:50:17.108049Z",
     "start_time": "2025-08-23T21:50:17.051989Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import inspect\n",
    "from collections import defaultdict\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# Helper: Encode with safe truncation and proper device\n",
    "# --------------------------------------------------------------\n",
    "def encode_trunc(text: str, tokenizer, device=\"cpu\", max_len=512):\n",
    "    kwargs = {\n",
    "        \"text\": text,\n",
    "        \"return_tensors\": \"pt\",\n",
    "        \"truncation\": True,\n",
    "        \"max_length\": max_len,\n",
    "    }\n",
    "    # Preserve your check for optional arg\n",
    "    sig = inspect.signature(tokenizer.__call__)\n",
    "    if \"add_special_tokens\" in sig.parameters:\n",
    "        kwargs[\"add_special_tokens\"] = True\n",
    "\n",
    "    enc = tokenizer(**kwargs)\n",
    "    # Force-move every tensor to the right device\n",
    "    for k, v in enc.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            enc[k] = v.to(device)\n",
    "    return enc\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# Augmentation with subwords\n",
    "# --------------------------------------------------------------\n",
    "def augment_with_subwords(\n",
    "    tokens: list[dict],\n",
    "    *,\n",
    "    tokenizer,\n",
    "    anchor_use_lemma: bool,\n",
    "    target_lemma: str\n",
    "):\n",
    "    target_lemma = target_lemma.lower()\n",
    "    aug_tokens = []\n",
    "    words = []\n",
    "    sp_tokens = []\n",
    "    sp_pos = 0\n",
    "\n",
    "    prepend = tokenizer.cls_token or \"<s>\"\n",
    "    append = tokenizer.sep_token or \"</s>\"\n",
    "    if prepend:\n",
    "        sp_tokens.append(prepend)\n",
    "        sp_pos += 1\n",
    "\n",
    "    for t in tokens:\n",
    "        is_anchor = t[\"lemma\"].lower() == target_lemma\n",
    "        word = t[\"lemma\"].lower() if is_anchor and anchor_use_lemma else t[\"token_text\"].lower()\n",
    "\n",
    "        try:\n",
    "            word_ids = tokenizer(word, add_special_tokens=False)[\"input_ids\"]\n",
    "        except Exception:\n",
    "            word_ids = tokenizer.encode(word, add_special_tokens=False)\n",
    "\n",
    "        subwords = tokenizer.convert_ids_to_tokens(word_ids)\n",
    "\n",
    "        new_t = dict(t)\n",
    "        new_t[\"sp_first\"] = sp_pos\n",
    "        new_t[\"sp_pieces\"] = subwords\n",
    "        aug_tokens.append(new_t)\n",
    "\n",
    "        words.append(word)\n",
    "        sp_tokens.extend(subwords)\n",
    "        sp_pos += len(subwords)\n",
    "\n",
    "    if append:\n",
    "        sp_tokens.append(append)\n",
    "\n",
    "    sent_str = \" \".join(words)\n",
    "    return sent_str, sp_tokens, aug_tokens\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# Hidden-state embedding from model\n",
    "# --------------------------------------------------------------\n",
    "def hidden_anchor_embedding(\n",
    "    aug_tokens: list[dict],\n",
    "    sent_str: str,\n",
    "    *,\n",
    "    tokenizer,\n",
    "    model,\n",
    "    device,\n",
    "    target_lemma: str,\n",
    "    layer_idx: int = 8,\n",
    "    piece_pooling: str = \"mean\",\n",
    "):\n",
    "    target_lemma = target_lemma.lower()\n",
    "    anchor = next((t for t in aug_tokens if t[\"lemma\"].lower() == target_lemma), None)\n",
    "    if anchor is None:\n",
    "        return np.zeros(model.config.hidden_size, dtype=np.float32)\n",
    "\n",
    "    sp_first = anchor[\"sp_first\"]\n",
    "    k = len(anchor[\"sp_pieces\"])\n",
    "\n",
    "    enc = encode_trunc(\n",
    "        sent_str,\n",
    "        tokenizer=tokenizer,\n",
    "        device=device,\n",
    "        max_len=tokenizer.model_max_length,\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outs = model(**enc, output_hidden_states=True)\n",
    "        hidden = outs.hidden_states[layer_idx].squeeze(0)  # [seq_len, dim]\n",
    "\n",
    "    if sp_first + k - 1 >= hidden.shape[0]:\n",
    "        return np.zeros(model.config.hidden_size, dtype=np.float32)\n",
    "\n",
    "    span = hidden[sp_first : sp_first + k]\n",
    "    vec = span.sum(dim=0) if piece_pooling == \"sum\" else span.mean(dim=0)\n",
    "    return vec.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# Attention-based weighting of lemmas\n",
    "# --------------------------------------------------------------\n",
    "def attention_weights_by_lemma(\n",
    "    aug_tokens: list[dict],\n",
    "    sent_str: str,\n",
    "    *,\n",
    "    tokenizer,\n",
    "    model,\n",
    "    device,\n",
    "    att_layer: int,\n",
    "    target_lemma: str,\n",
    "    top_k: int = 3,           # number of top attention heads to keep\n",
    "    direction: str = \"from\",  # \"from\" = target attends to others, \"to\" = others attend to target\n",
    "    normalize: bool = True,   # normalize anchor_vec to sum to 1\n",
    "):\n",
    "    target_lemma = target_lemma.lower()\n",
    "\n",
    "    enc = encode_trunc(\n",
    "        sent_str,\n",
    "        tokenizer=tokenizer,\n",
    "        device=device,\n",
    "        max_len=tokenizer.model_max_length,\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outs = model(**enc, output_attentions=True)\n",
    "        A_heads = outs.attentions[att_layer][0].cpu()  # [num_heads, L, L]\n",
    "\n",
    "    anchor = next((t for t in aug_tokens if t[\"lemma\"].lower() == target_lemma), None)\n",
    "    if anchor is None:\n",
    "        return {}\n",
    "\n",
    "    sp_first = anchor[\"sp_first\"]\n",
    "    k_anchor = len(anchor[\"sp_pieces\"])\n",
    "    L = A_heads.shape[-1]\n",
    "\n",
    "    if sp_first + k_anchor - 1 >= L:\n",
    "        return {}\n",
    "\n",
    "    if direction == \"from\":\n",
    "        per_head_anchor_vecs = A_heads[:, sp_first : sp_first + k_anchor, :].mean(dim=1)\n",
    "    elif direction == \"to\":\n",
    "        per_head_anchor_vecs = A_heads[:, :, sp_first : sp_first + k_anchor].mean(dim=2)\n",
    "    else:\n",
    "        raise ValueError(\"direction must be 'from' or 'to'\")\n",
    "\n",
    "    head_scores = per_head_anchor_vecs.sum(dim=1)  # [H]\n",
    "    top_head_ids = torch.topk(head_scores, k=top_k).indices\n",
    "    anchor_vec = per_head_anchor_vecs[top_head_ids].mean(dim=0)  # [L]\n",
    "\n",
    "    if normalize and anchor_vec.sum() > 0:\n",
    "        anchor_vec = anchor_vec / anchor_vec.sum()\n",
    "\n",
    "    lemma_info = defaultdict(lambda: {\"weight\": 0.0, \"pieces\": []})\n",
    "    for t in aug_tokens:\n",
    "        lemma = t[\"lemma\"].lower()\n",
    "        if lemma == target_lemma or lemma.strip() == \"\":\n",
    "            continue\n",
    "\n",
    "        start = t[\"sp_first\"]\n",
    "        end = start + len(t[\"sp_pieces\"])\n",
    "        if end > anchor_vec.shape[0]:\n",
    "            continue\n",
    "\n",
    "        total_w = anchor_vec[start:end].sum().item()\n",
    "        lemma_info[lemma][\"weight\"] += total_w\n",
    "\n",
    "        for j, piece in enumerate(t[\"sp_pieces\"]):\n",
    "            idx = start + j\n",
    "            lemma_info[lemma][\"pieces\"].append({\n",
    "                \"piece\": piece,\n",
    "                \"sp_idx\": idx,\n",
    "                \"weight\": float(anchor_vec[idx]),\n",
    "            })\n",
    "\n",
    "    return dict(lemma_info)"
   ],
   "id": "d921b330e25b271",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T21:50:17.156248Z",
     "start_time": "2025-08-23T21:50:17.152411Z"
    }
   },
   "cell_type": "code",
   "source": "tokens = enemy_annotations[\"tokens\"].tolist()[1]",
   "id": "c016d551f8da66be",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T21:50:17.203522Z",
     "start_time": "2025-08-23T21:50:17.200017Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sent_str, sp_tokens, aug_tokens = augment_with_subwords(\n",
    "    tokens,\n",
    "    tokenizer=tokenizer,\n",
    "    anchor_use_lemma=True,      # or False, per your design\n",
    "    target_lemma=\"ἐχθρός\",      # example\n",
    ")"
   ],
   "id": "e2e0e6a16b346114",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T21:50:17.254898Z",
     "start_time": "2025-08-23T21:50:17.248408Z"
    }
   },
   "cell_type": "code",
   "source": "aug_tokens",
   "id": "2fc0383697a8fedf",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'token_id': 152488913,\n",
       "  'token_text': 'οὐκ',\n",
       "  'lemma': 'οὐ',\n",
       "  'pos': 'r',\n",
       "  'char_start': 0,\n",
       "  'char_end': 3,\n",
       "  'sentence_id': 'lagt_tlg0061.tlg003_347',\n",
       "  'sp_first': 1,\n",
       "  'sp_pieces': ['ουκ']},\n",
       " {'token_id': 152488914,\n",
       "  'token_text': 'ἐχθρῶν',\n",
       "  'lemma': 'ἐχθρός',\n",
       "  'pos': 'a',\n",
       "  'char_start': 4,\n",
       "  'char_end': 10,\n",
       "  'sentence_id': 'lagt_tlg0061.tlg003_347',\n",
       "  'sp_first': 2,\n",
       "  'sp_pieces': ['εχθρος']},\n",
       " {'token_id': 152488915,\n",
       "  'token_text': 'ἡμῖν',\n",
       "  'lemma': 'ἡμεῖς',\n",
       "  'pos': 'p',\n",
       "  'char_start': 11,\n",
       "  'char_end': 15,\n",
       "  'sentence_id': 'lagt_tlg0061.tlg003_347',\n",
       "  'sp_first': 3,\n",
       "  'sp_pieces': ['ημι', '##ν']},\n",
       " {'token_id': 152488916,\n",
       "  'token_text': 'ἔχθιστος',\n",
       "  'lemma': 'ἔχθιστος',\n",
       "  'pos': 'a',\n",
       "  'char_start': 16,\n",
       "  'char_end': 24,\n",
       "  'sentence_id': 'lagt_tlg0061.tlg003_347',\n",
       "  'sp_first': 5,\n",
       "  'sp_pieces': ['εχ', '##θι', '##στος']},\n",
       " {'token_id': 152488917,\n",
       "  'token_text': 'ὁ',\n",
       "  'lemma': 'ὁ',\n",
       "  'pos': 'l',\n",
       "  'char_start': 25,\n",
       "  'char_end': 26,\n",
       "  'sentence_id': 'lagt_tlg0061.tlg003_347',\n",
       "  'sp_first': 8,\n",
       "  'sp_pieces': ['ο']},\n",
       " {'token_id': 152488918,\n",
       "  'token_text': 'Δημοσθένης',\n",
       "  'lemma': 'δημοσθένης',\n",
       "  'pos': 'n',\n",
       "  'char_start': 27,\n",
       "  'char_end': 37,\n",
       "  'sentence_id': 'lagt_tlg0061.tlg003_347',\n",
       "  'sp_first': 9,\n",
       "  'sp_pieces': ['δημοσθενη', '##ς']},\n",
       " {'token_id': 152488919,\n",
       "  'token_text': ';',\n",
       "  'lemma': ';',\n",
       "  'pos': 'u',\n",
       "  'char_start': 37,\n",
       "  'char_end': 38,\n",
       "  'sentence_id': 'lagt_tlg0061.tlg003_347',\n",
       "  'sp_first': 11,\n",
       "  'sp_pieces': [';']}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T21:50:17.365255Z",
     "start_time": "2025-08-23T21:50:17.349318Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- encode the full sentence for hidden states / attentions ---\n",
    "vec = hidden_anchor_embedding(\n",
    "    aug_tokens, sent_str,\n",
    "    tokenizer=tokenizer, model=model, device=\"cpu\",\n",
    "    target_lemma=\"ἐχθρός\", layer_idx=8, piece_pooling=\"mean\"\n",
    ")"
   ],
   "id": "1a1df9943fdacb90",
   "outputs": [],
   "execution_count": 17
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
